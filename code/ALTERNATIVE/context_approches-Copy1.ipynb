{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "import os\n",
    "from nltk import ngrams, word_tokenize\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../../Corpora/TO_CREATE_VALID_UTTERANCE/movies/chosen/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_alter import get_ngrams_from_sentences_given_in_lines, get_ngrams_from_sentences_given_in_lines2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_utils import get_top, get_closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peter_norvig_utils import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_context1_scan_and_compare(utterance, gramed):\n",
    "    old = False\n",
    "    approx_word = []\n",
    "    context = word_tokenize(utterance) \n",
    "    sub_grams, amount, word = utils(gramed, context)\n",
    "    \n",
    "    if not amount:\n",
    "        old = True\n",
    "    else:\n",
    "        top10 = [('',0)]\n",
    "        sub_grams_copy, sub_grams = itertools.tee(sub_grams)\n",
    "        vocab = iter({gram[1] for gram in sub_grams_copy})\n",
    "        for c in vocab:\n",
    "            if c[0] == word[0] or c[-1] == word[-1]:\n",
    "                p = sum([gramed[gram] for gram in sub_grams if gram[1] == c]) / amount\n",
    "                if top10[-1][1] < p:\n",
    "                    top10 = get_top(top10, c, p)\n",
    "        print(top10)\n",
    "\n",
    "        tokens, dist = zip(*top10)\n",
    "        approx_word = get_closest(tokens, word)\n",
    "    if old:\n",
    "#         vocab = skads\n",
    "#         aproxx = peter_norvig_approach(word, vocab)\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    return approx_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_context1_generate_and_scan(utterance, gramed):\n",
    "    aproxx = []\n",
    "    old = False\n",
    "    context = word_tokenize(utterance)\n",
    "    sub_grams, amount, word = utils(gramed, context)\n",
    "    \n",
    "    if not amount:\n",
    "        old = True\n",
    "    else:\n",
    "        vocab = {gram[1] for gram in sub_grams}\n",
    "        generated_phrases = generate(word)\n",
    "        valid_candidates = {phrase for phrase in generated_phrases if phrase in vocab}\n",
    "        if not valid_candidates:\n",
    "            old = True\n",
    "        else:\n",
    "            p_max = 0\n",
    "            aproxx = []\n",
    "            print(valid_candidates)\n",
    "            for el in valid_candidates:\n",
    "                    p = sum([gramed[gram] for gram in sub_grams if gram[1] == el]) / amount\n",
    "                    #a co jeżeli jest równe prawdopodobniestwo? popraw to\n",
    "                    if p_max < p:\n",
    "                        print(p, el)\n",
    "                        p_max = p\n",
    "                        aproxx = [el]\n",
    "                    elif p_max == p:\n",
    "                        aproxx.append(el)\n",
    "                        \n",
    "    if old:\n",
    "        pass\n",
    "#         vocab = skads\n",
    "#         aproxx = peter_norvig_approach(word, vocab)\n",
    "        \n",
    "    return aproxx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aproaches with defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_gram(n_gram):\n",
    "    lower_n_gram = defaultdict()\n",
    "    for w1 in n_gram:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_approach_bigrams(utterance, bi_grams):\n",
    "    w1, w2 = word_tokenize(utterance)\n",
    "    c_based_on_context = bi_grams[w1]\n",
    "    if c_based_on_context:\n",
    "        vocab = {c for c in c_based_on_context}\n",
    "        candidates = generate(w2)\n",
    "        valid_candidates = {phrase for phrase in candidates if phrase in vocab}\n",
    "        if valid_candidates:\n",
    "            p_max = 0\n",
    "            approx = []\n",
    "            for c in valid_candidates:\n",
    "                p = c_based_on_context[c]\n",
    "                if p_max < p:\n",
    "                    p_max = p\n",
    "                    approx = [c]\n",
    "                elif p_max == p:\n",
    "                    approx.append(c)\n",
    "            return approx[0]\n",
    "        \n",
    "    return \"xxxxxxx\"\n",
    "\n",
    "#     return without_context(utterance[1:], reduce_gram(bi_gram, w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_approach_trigrams(utterance, tri_grams):\n",
    "    w1, w2, w3 = word_tokenize(utterance)\n",
    "    c_based_on_context = tri_grams[w1][w2]\n",
    "    if c_based_on_context:\n",
    "        print(1)\n",
    "        vocab = {c for c in c_based_on_context}\n",
    "        candidates = generate(w3)\n",
    "        valid_candidates = {phrase for phrase in candidates if phrase in vocab}\n",
    "        if valid_candidates:\n",
    "            print(2)\n",
    "            p_max = 0\n",
    "            approx = []\n",
    "            for c in valid_candidates:\n",
    "                p = c_based_on_context[c]\n",
    "                if p_max < p:\n",
    "                    p_max = p\n",
    "                    approx = [c]\n",
    "                elif p_max == p:\n",
    "                    approx.append(c)\n",
    "            return approx[0]\n",
    "    \n",
    "    return \"xxxxx\"\n",
    "#     return context_approach_bigrams(utterance[1:], reduce_gram(tri_grams, w2))\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gramed1 = get_ngrams_from_sentences_given_in_lines(PATH, 3, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gramed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gramed2 = get_ngrams_from_sentences_given_in_lines2(PATH, 2, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramed2.items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_approach_bigrams(UTTERANCE, gramed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTTERANCE = \"information aboiu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_context1_scan_and_compare(UTTERANCE, gramed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_context1_generate_and_scan(UTTERANCE, gramed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bi_grams_opensubtitles\", \"bw\") as handle:\n",
    "    pickle.dump(gramed2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tri_grams_google_one_bilions_generator\", \"br\") as handle:\n",
    "    gramed = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from multiprocessing.managers import BaseManager, DictProxy\n",
    "from collections import defaultdict\n",
    "\n",
    "class MyManager(BaseManager):\n",
    "    pass\n",
    "\n",
    "MyManager.register('defaultdict', defaultdict, DictProxy)\n",
    "\n",
    "def test(k, multi_dict):\n",
    "    multi_dict[k] += 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = Pool(processes=4)\n",
    "    mgr = MyManager()\n",
    "    mgr.start()\n",
    "    multi_d = mgr.defaultdict(lambda: defaultdict(int))\n",
    "    for k in 'mississippi':\n",
    "        pool.apply_async(test, (k, multi_d))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print (multi_d.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dd():\n",
    "    return defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = defaultdict(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd[\"s\"][\"s\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = defaultdict(lambda: defaultdict(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
